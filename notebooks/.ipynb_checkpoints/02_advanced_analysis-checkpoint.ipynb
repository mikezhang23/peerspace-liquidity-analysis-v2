{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ab0c7-0929-4181-b059-1b1908eb1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis: Predictive Modeling & Market Segmentation\n",
    "# This notebook builds on SQL analysis to create predictive models and actionable segments\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADVANCED MARKETPLACE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 1: DATA PREPARATION\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n1. DATA PREPARATION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect('../data/peerspace_marketplace.db')\n",
    "\n",
    "# Load all tables with proper date parsing\n",
    "listings_df = pd.read_sql(\"SELECT * FROM listings\", conn)\n",
    "searches_df = pd.read_sql(\"SELECT * FROM searches\", conn)\n",
    "bookings_df = pd.read_sql(\"SELECT * FROM bookings\", conn)\n",
    "users_df = pd.read_sql(\"SELECT * FROM users\", conn)\n",
    "\n",
    "# Convert date columns\n",
    "date_columns = {\n",
    "    'listings_df': ['created_date'],\n",
    "    'searches_df': ['search_date', 'event_date'],\n",
    "    'bookings_df': ['booking_date', 'event_date'],\n",
    "    'users_df': ['signup_date']\n",
    "}\n",
    "\n",
    "for df_name, cols in date_columns.items():\n",
    "    df = locals()[df_name]\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "print(f\"âœ“ Loaded data: {len(listings_df)} venues, {len(searches_df)} searches, {len(bookings_df)} bookings\")\n",
    "\n",
    "# Create enriched datasets for modeling\n",
    "# 1. Venue Performance Dataset\n",
    "venue_performance = listings_df.merge(\n",
    "    bookings_df.groupby('venue_id').agg({\n",
    "        'booking_id': 'count',\n",
    "        'total_amount': ['sum', 'mean'],\n",
    "        'hours_booked': ['sum', 'mean']\n",
    "    }).reset_index(),\n",
    "    how='left',\n",
    "    on='venue_id'\n",
    ")\n",
    "venue_performance.columns = ['venue_id', 'host_id', 'metro_area', 'venue_type', \n",
    "                             'price_per_hour', 'capacity', 'created_date', \n",
    "                             'listing_title', 'is_active', 'total_bookings',\n",
    "                             'revenue_total', 'revenue_avg', 'hours_total', 'hours_avg']\n",
    "venue_performance.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate venue age and utilization\n",
    "venue_performance['venue_age_days'] = (pd.Timestamp('2024-12-31') - venue_performance['created_date']).dt.days\n",
    "venue_performance['utilization_rate'] = (venue_performance['hours_total'] / (venue_performance['venue_age_days'] * 10)) * 100\n",
    "venue_performance['utilization_rate'] = venue_performance['utilization_rate'].clip(upper=100)\n",
    "\n",
    "print(\"âœ“ Created venue performance dataset\")\n",
    "\n",
    "# 2. Search Conversion Dataset\n",
    "search_conversion = searches_df.merge(\n",
    "    bookings_df[['search_id', 'booking_id', 'status']],\n",
    "    how='left',\n",
    "    on='search_id'\n",
    ")\n",
    "search_conversion['converted'] = (~search_conversion['booking_id'].isna()).astype(int)\n",
    "search_conversion['lead_time_days'] = (search_conversion['event_date'] - search_conversion['search_date']).dt.days\n",
    "search_conversion['search_month'] = search_conversion['search_date'].dt.month\n",
    "search_conversion['search_weekday'] = search_conversion['search_date'].dt.dayofweek\n",
    "search_conversion['event_weekday'] = search_conversion['event_date'].dt.dayofweek\n",
    "\n",
    "print(\"âœ“ Created search conversion dataset\")\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 2: LIQUIDITY HEALTH SCORE PREDICTION\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n2. LIQUIDITY SCORE PREDICTION MODEL\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Aggregate monthly liquidity metrics by metro\n",
    "monthly_liquidity = []\n",
    "\n",
    "for month in pd.date_range('2024-01-01', '2024-12-01', freq='MS'):\n",
    "    month_end = month + pd.DateOffset(months=1) - pd.DateOffset(days=1)\n",
    "    \n",
    "    for metro in listings_df['metro_area'].unique():\n",
    "        # Supply metrics\n",
    "        active_venues = listings_df[\n",
    "            (listings_df['metro_area'] == metro) & \n",
    "            (listings_df['created_date'] <= month_end) &\n",
    "            (listings_df['is_active'] == True)\n",
    "        ].shape[0]\n",
    "        \n",
    "        # Demand metrics\n",
    "        month_searches = searches_df[\n",
    "            (searches_df['metro_area'] == metro) &\n",
    "            (searches_df['search_date'] >= month) &\n",
    "            (searches_df['search_date'] <= month_end)\n",
    "        ]\n",
    "        \n",
    "        search_count = month_searches.shape[0]\n",
    "        conversion_rate = month_searches['search_resulted_in_booking'].mean() * 100 if search_count > 0 else 0\n",
    "        \n",
    "        # Price metrics\n",
    "        avg_search_price = month_searches['max_price'].mean() if search_count > 0 else 0\n",
    "        avg_venue_price = listings_df[listings_df['metro_area'] == metro]['price_per_hour'].mean()\n",
    "        \n",
    "        # Calculate liquidity score (simplified version)\n",
    "        if active_venues > 0 and search_count > 0:\n",
    "            liquidity_score = (\n",
    "                conversion_rate * 0.4 +\n",
    "                min((search_count / active_venues) * 10, 100) * 0.3 +\n",
    "                min((active_venues / 20), 1) * 100 * 0.3\n",
    "            )\n",
    "        else:\n",
    "            liquidity_score = 0\n",
    "            \n",
    "        monthly_liquidity.append({\n",
    "            'month': month,\n",
    "            'metro_area': metro,\n",
    "            'active_venues': active_venues,\n",
    "            'search_count': search_count,\n",
    "            'conversion_rate': conversion_rate,\n",
    "            'avg_search_price': avg_search_price,\n",
    "            'avg_venue_price': avg_venue_price,\n",
    "            'liquidity_score': liquidity_score\n",
    "        })\n",
    "\n",
    "liquidity_df = pd.DataFrame(monthly_liquidity)\n",
    "\n",
    "# Create lagged features for time series prediction\n",
    "for lag in [1, 2, 3]:\n",
    "    liquidity_df[f'liquidity_lag_{lag}'] = liquidity_df.groupby('metro_area')['liquidity_score'].shift(lag)\n",
    "    liquidity_df[f'searches_lag_{lag}'] = liquidity_df.groupby('metro_area')['search_count'].shift(lag)\n",
    "    liquidity_df[f'venues_lag_{lag}'] = liquidity_df.groupby('metro_area')['active_venues'].shift(lag)\n",
    "\n",
    "# Add trend features\n",
    "liquidity_df['month_num'] = (liquidity_df['month'].dt.year - 2024) * 12 + liquidity_df['month'].dt.month\n",
    "liquidity_df['is_summer'] = liquidity_df['month'].dt.month.isin([6, 7, 8]).astype(int)\n",
    "liquidity_df['is_holiday_season'] = liquidity_df['month'].dt.month.isin([11, 12]).astype(int)\n",
    "\n",
    "# Clean dataset (remove NaN from lagged features)\n",
    "liquidity_model_df = liquidity_df.dropna()\n",
    "\n",
    "print(f\"âœ“ Created liquidity dataset with {len(liquidity_model_df)} monthly observations\")\n",
    "\n",
    "# Build prediction model\n",
    "feature_cols = ['active_venues', 'search_count', 'conversion_rate', 'avg_search_price', \n",
    "                'avg_venue_price', 'liquidity_lag_1', 'liquidity_lag_2', 'liquidity_lag_3',\n",
    "                'searches_lag_1', 'venues_lag_1', 'month_num', 'is_summer', 'is_holiday_season']\n",
    "\n",
    "X = liquidity_model_df[feature_cols]\n",
    "y = liquidity_model_df['liquidity_score']\n",
    "\n",
    "# Time series split (don't shuffle for time series!)\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nðŸ“Š Liquidity Score Prediction Results:\")\n",
    "print(f\"   MAE: {mae:.2f}\")\n",
    "print(f\"   RMSE: {rmse:.2f}\")\n",
    "print(f\"   RÂ²: {r2:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "ax1.scatter(y_test, y_pred, alpha=0.6)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Liquidity Score')\n",
    "ax1.set_ylabel('Predicted Liquidity Score')\n",
    "ax1.set_title(f'Liquidity Score Predictions (RÂ² = {r2:.3f})')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Importance\n",
    "ax2.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Top 10 Feature Importances')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Top 3 predictive features:\")\n",
    "for idx, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 3: PRICE ELASTICITY ANALYSIS\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n3. PRICE ELASTICITY MODELING\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Prepare price elasticity dataset\n",
    "price_data = searches_df.merge(\n",
    "    listings_df[['metro_area', 'venue_type', 'price_per_hour', 'capacity']],\n",
    "    on=['metro_area', 'venue_type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate if search converted based on price\n",
    "price_data['price_match'] = (price_data['price_per_hour'] <= price_data['max_price']).astype(int)\n",
    "price_data['capacity_match'] = (price_data['capacity'] >= price_data['capacity_needed'] * 0.8).astype(int)\n",
    "\n",
    "# Group by venue type and price bands\n",
    "elasticity_results = []\n",
    "\n",
    "for venue_type in price_data['venue_type'].unique():\n",
    "    type_data = price_data[price_data['venue_type'] == venue_type].copy()\n",
    "    \n",
    "    if len(type_data) < 50:  # Skip if too few observations\n",
    "        continue\n",
    "    \n",
    "    # Create price bands (quartiles)\n",
    "    type_data['price_band'] = pd.qcut(type_data['price_per_hour'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "    \n",
    "    # Calculate conversion by price band\n",
    "    band_conversion = type_data.groupby('price_band').agg({\n",
    "        'search_resulted_in_booking': 'mean',\n",
    "        'price_per_hour': 'mean',\n",
    "        'search_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate elasticity (% change in demand / % change in price)\n",
    "    if len(band_conversion) > 1:\n",
    "        prices = band_conversion['price_per_hour'].values\n",
    "        conversions = band_conversion['search_resulted_in_booking'].values\n",
    "        \n",
    "        # Simple elasticity calculation\n",
    "        price_pct_change = (prices[-1] - prices[0]) / prices[0]\n",
    "        demand_pct_change = (conversions[-1] - conversions[0]) / conversions[0] if conversions[0] > 0 else 0\n",
    "        elasticity = demand_pct_change / price_pct_change if price_pct_change != 0 else 0\n",
    "        \n",
    "        elasticity_results.append({\n",
    "            'venue_type': venue_type,\n",
    "            'elasticity': elasticity,\n",
    "            'avg_price': prices.mean(),\n",
    "            'price_range': prices[-1] - prices[0],\n",
    "            'sample_size': type_data.shape[0]\n",
    "        })\n",
    "\n",
    "elasticity_df = pd.DataFrame(elasticity_results)\n",
    "\n",
    "print(\"âœ“ Price Elasticity by Venue Type:\")\n",
    "for idx, row in elasticity_df.iterrows():\n",
    "    elasticity_type = \"Elastic\" if abs(row['elasticity']) > 1 else \"Inelastic\"\n",
    "    print(f\"   {row['venue_type']}: {row['elasticity']:.2f} ({elasticity_type})\")\n",
    "\n",
    "# Visualize price elasticity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elasticity by venue type\n",
    "ax1.barh(elasticity_df['venue_type'], elasticity_df['elasticity'].abs())\n",
    "ax1.set_xlabel('Price Elasticity (Absolute Value)')\n",
    "ax1.set_title('Price Sensitivity by Venue Type')\n",
    "ax1.axvline(x=1, color='r', linestyle='--', alpha=0.5, label='Elastic/Inelastic Boundary')\n",
    "ax1.legend()\n",
    "\n",
    "# Price vs Conversion for most elastic type\n",
    "most_elastic = elasticity_df.loc[elasticity_df['elasticity'].abs().idxmax(), 'venue_type']\n",
    "elastic_data = price_data[price_data['venue_type'] == most_elastic].copy()\n",
    "\n",
    "# Bin prices and calculate conversion\n",
    "elastic_data['price_bin'] = pd.cut(elastic_data['price_per_hour'], bins=10)\n",
    "price_conversion = elastic_data.groupby('price_bin').agg({\n",
    "    'search_resulted_in_booking': 'mean',\n",
    "    'price_per_hour': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "ax2.scatter(price_conversion['price_per_hour'], \n",
    "           price_conversion['search_resulted_in_booking'] * 100)\n",
    "ax2.set_xlabel('Price per Hour ($)')\n",
    "ax2.set_ylabel('Conversion Rate (%)')\n",
    "ax2.set_title(f'Price vs Conversion: {most_elastic} (Most Elastic)')\n",
    "\n",
    "# Fit trend line\n",
    "z = np.polyfit(price_conversion['price_per_hour'].dropna(), \n",
    "               price_conversion['search_resulted_in_booking'].dropna() * 100, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(price_conversion['price_per_hour'], \n",
    "         p(price_conversion['price_per_hour']), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 4: MARKET SEGMENTATION\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n4. MARKET SEGMENTATION ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Create metro-level features for clustering\n",
    "metro_features = pd.DataFrame()\n",
    "\n",
    "for metro in listings_df['metro_area'].unique():\n",
    "    metro_listings = listings_df[listings_df['metro_area'] == metro]\n",
    "    metro_searches = searches_df[searches_df['metro_area'] == metro]\n",
    "    metro_bookings = bookings_df[bookings_df['venue_id'].isin(metro_listings['venue_id'])]\n",
    "    \n",
    "    metro_features = pd.concat([metro_features, pd.DataFrame({\n",
    "        'metro_area': [metro],\n",
    "        'supply_count': [len(metro_listings)],\n",
    "        'demand_count': [len(metro_searches)],\n",
    "        'booking_count': [len(metro_bookings)],\n",
    "        'avg_price': [metro_listings['price_per_hour'].mean()],\n",
    "        'price_std': [metro_listings['price_per_hour'].std()],\n",
    "        'conversion_rate': [metro_searches['search_resulted_in_booking'].mean()],\n",
    "        'avg_lead_time': [(metro_searches['event_date'] - metro_searches['search_date']).dt.days.mean()],\n",
    "        'venue_diversity': [metro_listings['venue_type'].nunique()],\n",
    "        'supply_demand_ratio': [len(metro_listings) / len(metro_searches) if len(metro_searches) > 0 else 0],\n",
    "        'avg_booking_value': [metro_bookings['total_amount'].mean() if len(metro_bookings) > 0 else 0]\n",
    "    })])\n",
    "\n",
    "metro_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(metro_features.drop('metro_area', axis=1))\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "metro_features['cluster'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Interpret clusters\n",
    "cluster_profiles = metro_features.groupby('cluster').mean()\n",
    "\n",
    "print(\"âœ“ Market Segments Identified:\")\n",
    "for cluster_id in range(3):\n",
    "    cluster_metros = metro_features[metro_features['cluster'] == cluster_id]['metro_area'].values\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    \n",
    "    # Determine cluster characteristics\n",
    "    if profile['conversion_rate'] > 0.4 and profile['supply_demand_ratio'] < 0.15:\n",
    "        segment_name = \"High-Performance Markets\"\n",
    "        strategy = \"Maintain balance, optimize pricing\"\n",
    "    elif profile['supply_count'] > 100 and profile['conversion_rate'] < 0.35:\n",
    "        segment_name = \"Oversupplied Markets\"\n",
    "        strategy = \"Demand generation, host retention\"\n",
    "    else:\n",
    "        segment_name = \"Growth Opportunity Markets\"\n",
    "        strategy = \"Supply acquisition, market education\"\n",
    "    \n",
    "    print(f\"\\n   Segment {cluster_id + 1}: {segment_name}\")\n",
    "    print(f\"   Metros: {', '.join(cluster_metros)}\")\n",
    "    print(f\"   Strategy: {strategy}\")\n",
    "    print(f\"   Avg Conversion: {profile['conversion_rate']:.1%}\")\n",
    "    print(f\"   Supply/Demand: {profile['supply_demand_ratio']:.3f}\")\n",
    "\n",
    "# Visualize clusters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA visualization\n",
    "scatter = ax1.scatter(features_pca[:, 0], features_pca[:, 1], \n",
    "                     c=metro_features['cluster'], cmap='viridis', s=100)\n",
    "for i, metro in enumerate(metro_features['metro_area']):\n",
    "    ax1.annotate(metro, (features_pca[i, 0], features_pca[i, 1]), fontsize=9)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.set_title('Market Segmentation (PCA Visualization)')\n",
    "plt.colorbar(scatter, ax=ax1)\n",
    "\n",
    "# Cluster characteristics radar chart\n",
    "categories = ['Supply', 'Demand', 'Conversion', 'Price', 'Diversity']\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    values = [\n",
    "        cluster_profiles.loc[cluster_id, 'supply_count'] / cluster_profiles['supply_count'].max(),\n",
    "        cluster_profiles.loc[cluster_id, 'demand_count'] / cluster_profiles['demand_count'].max(),\n",
    "        cluster_profiles.loc[cluster_id, 'conversion_rate'] / cluster_profiles['conversion_rate'].max(),\n",
    "        cluster_profiles.loc[cluster_id, 'avg_price'] / cluster_profiles['avg_price'].max(),\n",
    "        cluster_profiles.loc[cluster_id, 'venue_diversity'] / cluster_profiles['venue_diversity'].max()\n",
    "    ]\n",
    "    values = np.concatenate((values, [values[0]]))\n",
    "    \n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label=f'Segment {cluster_id + 1}')\n",
    "    ax2.fill(angles, values, alpha=0.25)\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.set_title('Segment Characteristics Comparison')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 5: DEMAND FORECASTING\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n5. DEMAND FORECASTING MODEL\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Prepare time series data for forecasting\n",
    "daily_demand = searches_df.groupby('search_date').agg({\n",
    "    'search_id': 'count',\n",
    "    'search_resulted_in_booking': 'mean'\n",
    "}).reset_index()\n",
    "daily_demand.columns = ['date', 'search_count', 'conversion_rate']\n",
    "daily_demand.set_index('date', inplace=True)\n",
    "\n",
    "# Fill missing dates\n",
    "idx = pd.date_range(daily_demand.index.min(), daily_demand.index.max())\n",
    "daily_demand = daily_demand.reindex(idx, fill_value=0)\n",
    "\n",
    "# Apply moving average for smoothing\n",
    "daily_demand['search_count_ma7'] = daily_demand['search_count'].rolling(window=7).mean()\n",
    "\n",
    "# Decompose time series\n",
    "decomposition = seasonal_decompose(daily_demand['search_count'].fillna(method='ffill'), \n",
    "                                  model='additive', period=7)\n",
    "\n",
    "# Train Exponential Smoothing model\n",
    "train_size = int(len(daily_demand) * 0.8)\n",
    "train, test = daily_demand['search_count'][:train_size], daily_demand['search_count'][train_size:]\n",
    "\n",
    "# Fit model\n",
    "model = ExponentialSmoothing(train, seasonal='add', seasonal_periods=7)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_fit.forecast(len(test))\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': test.values,\n",
    "    'predicted': predictions.values\n",
    "}, index=test.index)\n",
    "\n",
    "# Calculate forecast accuracy\n",
    "forecast_mae = mean_absolute_error(predictions_df['actual'], predictions_df['predicted'])\n",
    "forecast_rmse = np.sqrt(mean_squared_error(predictions_df['actual'], predictions_df['predicted']))\n",
    "\n",
    "print(f\"âœ“ Demand Forecast Accuracy:\")\n",
    "print(f\"   MAE: {forecast_mae:.2f} searches/day\")\n",
    "print(f\"   RMSE: {forecast_rmse:.2f} searches/day\")\n",
    "\n",
    "# Future forecast (next 30 days)\n",
    "future_forecast = model_fit.forecast(30)\n",
    "future_dates = pd.date_range(start=daily_demand.index[-1] + timedelta(days=1), periods=30)\n",
    "\n",
    "# Visualize forecasts\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series decomposition\n",
    "ax1.plot(daily_demand.index, daily_demand['search_count'], label='Actual', alpha=0.5)\n",
    "ax1.plot(daily_demand.index, daily_demand['search_count_ma7'], label='7-day MA', linewidth=2)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Daily Searches')\n",
    "ax1.set_title('Search Demand Over Time')\n",
    "ax1.legend()\n",
    "\n",
    "# Seasonal pattern\n",
    "ax2.plot(decomposition.seasonal[:30])\n",
    "ax2.set_xlabel('Day of Month')\n",
    "ax2.set_ylabel('Seasonal Component')\n",
    "ax2.set_title('Weekly Seasonality Pattern')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast vs Actual\n",
    "ax3.plot(test.index, test.values, label='Actual', marker='o', markersize=3)\n",
    "ax3.plot(predictions_df.index, predictions_df['predicted'], \n",
    "         label='Predicted', marker='s', markersize=3, alpha=0.7)\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Daily Searches')\n",
    "ax3.set_title(f'Forecast Validation (MAE: {forecast_mae:.1f})')\n",
    "ax3.legend()\n",
    "\n",
    "# Future forecast\n",
    "ax4.plot(daily_demand.index[-60:], daily_demand['search_count'][-60:], \n",
    "         label='Historical', marker='o', markersize=3)\n",
    "ax4.plot(future_dates, future_forecast, label='30-day Forecast', \n",
    "         marker='^', markersize=4, color='red')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Daily Searches')\n",
    "ax4.set_title('30-Day Demand Forecast')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Next 30 days forecast:\")\n",
    "print(f\"   Expected total searches: {future_forecast.sum():.0f}\")\n",
    "print(f\"   Daily average: {future_forecast.mean():.1f}\")\n",
    "print(f\"   Peak day: {future_dates[future_forecast.argmax()].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# =====================================================\n",
    "# SECTION 6: CONVERSION PREDICTION MODEL\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n6. SEARCH CONVERSION PREDICTION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Prepare features for conversion prediction\n",
    "conversion_features = search_conversion[['metro_area', 'venue_type', 'capacity_needed', \n",
    "                                        'max_price', 'lead_time_days', 'search_month',\n",
    "                                        'search_weekday', 'event_weekday', 'converted']].copy()\n",
    "\n",
    "# Add supply availability features\n",
    "for idx, row in conversion_features.iterrows():\n",
    "    matching_venues = listings_df[\n",
    "        (listings_df['metro_area'] == row['metro_area']) &\n",
    "        (listings_df['venue_type'] == row['venue_type']) &\n",
    "        (listings_df['capacity'] >= row['capacity_needed'] * 0.8) &\n",
    "        (listings_df['price_per_hour'] <= row['max_price'])\n",
    "    ]\n",
    "    conversion_features.loc[idx, 'matching_venues'] = len(matching_venues)\n",
    "    conversion_features.loc[idx, 'avg_matching_price'] = matching_venues['price_per_hour'].mean() if len(matching_venues) > 0 else 0\n",
    "\n",
    "# Encode categorical variables\n",
    "le_metro = LabelEncoder()\n",
    "le_venue = LabelEncoder()\n",
    "conversion_features['metro_encoded'] = le_metro.fit_transform(conversion_features['metro_area'])\n",
    "conversion_features['venue_encoded'] = le_venue.fit_transform(conversion_features['venue_type'])\n",
    "\n",
    "# Select features for model\n",
    "feature_cols = ['metro_encoded', 'venue_encoded', 'capacity_needed', 'max_price',\n",
    "                'lead_time_days', 'search_month', 'search_weekday', 'event_weekday',\n",
    "                'matching_venues', 'avg_matching_price']\n",
    "\n",
    "X = conversion_features[feature_cols]\n",
    "y = conversion_features['converted']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ“ Conversion Prediction Model Performance:\")\n",
    "print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall: {recall:.3f}\")\n",
    "print(f\"   F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_conv = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nâœ“ Top factors affecting conversion:\")\n",
    "for idx, row in feature_importance_conv.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# =====================================================\n",
    "# SUMMARY & RECOMMENDATIONS\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\n1. PREDICTIVE CAPABILITIES:\")\n",
    "print(f\"   â€¢ Liquidity score prediction RÂ²: {r2:.3f}\")\n",
    "print(f\"   â€¢ Demand forecast accuracy (MAE): {forecast_mae:.1f} searches/day\")\n",
    "print(f\"   â€¢ Conversion prediction accuracy: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\n2. MARKET SEGMENTS:\")\n",
    "for cluster_id in range(3):\n",
    "    cluster_metros = metro_features[metro_features['cluster'] == cluster_id]['metro_area'].values\n",
    "    print(f\"   â€¢ Segment {cluster_id + 1}: {', '.join(cluster_metros)}\")\n",
    "\n",
    "print(\"\\n3. PRICE ELASTICITY FINDINGS:\")\n",
    "elastic_venues = elasticity_df[elasticity_df['elasticity'].abs() > 1]['venue_type'].values\n",
    "if len(elastic_venues) > 0:\n",
    "    print(f\"   â€¢ Price-sensitive venues: {', '.join(elastic_venues)}\")\n",
    "    print(f\"   â€¢ Recommendation: Dynamic pricing for elastic categories\")\n",
    "\n",
    "print(\"\\n4. DEMAND PATTERNS:\")\n",
    "print(f\"   â€¢ Weekly seasonality detected with {decomposition.seasonal[:7].max():.1f} peak variation\")\n",
    "print(f\"   â€¢ Next 30 days expected demand: {future_forecast.sum():.0f} searches\")\n",
    "\n",
    "print(\"\\nðŸ“Š ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Generate specific recommendations based on analysis\n",
    "recommendations = []\n",
    "\n",
    "# 1. Based on liquidity prediction\n",
    "low_liquidity_metros = liquidity_df.groupby('metro_area')['liquidity_score'].mean().sort_values().head(2)\n",
    "for metro, score in low_liquidity_metros.items():\n",
    "    recommendations.append({\n",
    "        'metro': metro,\n",
    "        'issue': 'Low Liquidity',\n",
    "        'action': f'Increase supply by {int((50-score)/2)}% or boost marketing by {int((50-score)*1.5)}%',\n",
    "        'priority': 'High',\n",
    "        'expected_impact': f'${int((50-score)*5000)} additional monthly revenue'\n",
    "    })\n",
    "\n",
    "# 2. Based on price elasticity\n",
    "for idx, row in elasticity_df.iterrows():\n",
    "    if abs(row['elasticity']) > 1.5:\n",
    "        recommendations.append({\n",
    "            'metro': 'All',\n",
    "            'issue': f'{row[\"venue_type\"]} high price sensitivity',\n",
    "            'action': f'Implement dynamic pricing with Â±15% range',\n",
    "            'priority': 'Medium',\n",
    "            'expected_impact': f'{abs(row[\"elasticity\"]*10):.0f}% conversion improvement'\n",
    "        })\n",
    "\n",
    "# 3. Based on segmentation\n",
    "for cluster_id in range(3):\n",
    "    cluster_data = metro_features[metro_features['cluster'] == cluster_id]\n",
    "    if cluster_data['conversion_rate'].mean() < 0.35:\n",
    "        for metro in cluster_data['metro_area']:\n",
    "            recommendations.append({\n",
    "                'metro': metro,\n",
    "                'issue': 'Poor Conversion',\n",
    "                'action': 'Improve venue-search matching algorithm',\n",
    "                'priority': 'High',\n",
    "                'expected_impact': '20% reduction in unfulfilled searches'\n",
    "            })\n",
    "\n",
    "# Create recommendations dataframe\n",
    "recommendations_df = pd.DataFrame(recommendations).drop_duplicates(subset=['metro', 'issue'])\n",
    "\n",
    "print(\"\\nðŸ“‹ PRIORITIZED ACTION PLAN:\")\n",
    "for priority in ['High', 'Medium', 'Low']:\n",
    "    priority_recs = recommendations_df[recommendations_df['priority'] == priority]\n",
    "    if len(priority_recs) > 0:\n",
    "        print(f\"\\n{priority} Priority:\")\n",
    "        for idx, rec in priority_recs.iterrows():\n",
    "            print(f\"   â€¢ {rec['metro']}: {rec['action']}\")\n",
    "            print(f\"     Expected Impact: {rec['expected_impact']}\")\n",
    "\n",
    "# =====================================================\n",
    "# EXPORT RESULTS FOR DASHBOARD\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save key dataframes for Tableau\n",
    "liquidity_df.to_csv('../data/liquidity_predictions.csv', index=False)\n",
    "metro_features.to_csv('../data/metro_segments.csv', index=False)\n",
    "elasticity_df.to_csv('../data/price_elasticity.csv', index=False)\n",
    "recommendations_df.to_csv('../data/recommendations.csv', index=False)\n",
    "\n",
    "# Save model predictions\n",
    "predictions_export = pd.DataFrame({\n",
    "    'date': test.index,\n",
    "    'actual_searches': test.values,\n",
    "    'predicted_searches': predictions.values\n",
    "})\n",
    "predictions_export.to_csv('../data/demand_forecast.csv', index=False)\n",
    "\n",
    "# Save future forecast\n",
    "future_forecast_df = pd.DataFrame({\n",
    "    'date': future_dates,\n",
    "    'forecasted_searches': future_forecast\n",
    "})\n",
    "future_forecast_df.to_csv('../data/future_demand_forecast.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Exported files:\")\n",
    "print(\"   â€¢ liquidity_predictions.csv - Monthly liquidity scores and predictions\")\n",
    "print(\"   â€¢ metro_segments.csv - Market segmentation results\")\n",
    "print(\"   â€¢ price_elasticity.csv - Price sensitivity analysis\")\n",
    "print(\"   â€¢ recommendations.csv - Prioritized action items\")\n",
    "print(\"   â€¢ demand_forecast.csv - Historical forecast validation\")\n",
    "print(\"   â€¢ future_demand_forecast.csv - 30-day forward forecast\")\n",
    "\n",
    "# =====================================================\n",
    "# FINAL VISUALIZATION SUMMARY\n",
    "# =====================================================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Liquidity Score by Metro (Current vs Predicted)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "latest_liquidity = liquidity_df.groupby('metro_area')['liquidity_score'].last().sort_values()\n",
    "ax1.barh(latest_liquidity.index, latest_liquidity.values, color='skyblue')\n",
    "ax1.set_xlabel('Liquidity Score')\n",
    "ax1.set_title('Current Liquidity Scores by Metro', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(x=50, color='r', linestyle='--', alpha=0.5, label='Threshold')\n",
    "\n",
    "# 2. Conversion Funnel\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "funnel_stages = ['Searches', 'Matched', 'Converted', 'Completed']\n",
    "funnel_values = [\n",
    "    len(searches_df),\n",
    "    len(searches_df[searches_df['search_resulted_in_booking'] == 1]),\n",
    "    len(bookings_df),\n",
    "    len(bookings_df[bookings_df['status'] == 'completed'])\n",
    "]\n",
    "ax2.barh(range(len(funnel_stages)), funnel_values, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
    "ax2.set_yticks(range(len(funnel_stages)))\n",
    "ax2.set_yticklabels(funnel_stages)\n",
    "ax2.set_xlabel('Count')\n",
    "ax2.set_title('Conversion Funnel', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Price Elasticity\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "elasticity_colors = ['red' if abs(e) > 1 else 'green' for e in elasticity_df['elasticity']]\n",
    "ax3.bar(range(len(elasticity_df)), elasticity_df['elasticity'].abs(), color=elasticity_colors)\n",
    "ax3.set_xticks(range(len(elasticity_df)))\n",
    "ax3.set_xticklabels(elasticity_df['venue_type'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Price Elasticity')\n",
    "ax3.set_title('Price Sensitivity by Venue Type', fontsize=12, fontweight='bold')\n",
    "ax3.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Market Segmentation\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "segment_sizes = metro_features['cluster'].value_counts().sort_index()\n",
    "colors_seg = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "ax4.pie(segment_sizes, labels=[f'Segment {i+1}' for i in range(3)], \n",
    "        autopct='%1.0f%%', colors=colors_seg)\n",
    "ax4.set_title('Market Segmentation', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. Demand Forecast\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.plot(daily_demand.index[-90:], daily_demand['search_count'][-90:], \n",
    "         label='Historical', alpha=0.7, linewidth=1)\n",
    "ax5.plot(future_dates, future_forecast, label='30-day Forecast', \n",
    "         color='red', linewidth=2, marker='o', markersize=3)\n",
    "ax5.fill_between(future_dates, \n",
    "                  future_forecast * 0.8, \n",
    "                  future_forecast * 1.2, \n",
    "                  alpha=0.3, color='red', label='Confidence Interval')\n",
    "ax5.set_xlabel('Date')\n",
    "ax5.set_ylabel('Daily Searches')\n",
    "ax5.set_title('Demand Forecast - Next 30 Days', fontsize=12, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MARKETPLACE LIQUIDITY ADVANCED ANALYTICS DASHBOARD', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.savefig('../data/advanced_analysis_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Summary dashboard saved as 'advanced_analysis_summary.png'\")\n",
    "\n",
    "# =====================================================\n",
    "# MODEL PERSISTENCE\n",
    "# =====================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save trained models for future use\n",
    "models = {\n",
    "    'liquidity_predictor': rf_model,\n",
    "    'conversion_predictor': rf_classifier,\n",
    "    'demand_forecaster': model_fit,\n",
    "    'label_encoder_metro': le_metro,\n",
    "    'label_encoder_venue': le_venue,\n",
    "    'feature_scaler': scaler\n",
    "}\n",
    "\n",
    "for model_name, model_obj in models.items():\n",
    "    with open(f'../data/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(model_obj, f)\n",
    "\n",
    "print(\"\\nâœ… Trained models saved for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Review the generated visualizations and insights\")\n",
    "print(\"2. Import CSV files into Tableau for interactive dashboard\")\n",
    "print(\"3. Use the recommendations.csv for executive presentation\")\n",
    "print(\"4. Deploy models using the saved .pkl files\")\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
